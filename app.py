#!/usr/bin/env python3
"""
Professional Journalist Script Generator
Ù†Ø¸Ø§Ù… Ø§Ø­ØªØ±Ø§ÙÙŠ Ù„ØªÙˆÙ„ÙŠØ¯ Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù…
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import json
import os
import random
from datetime import datetime
from collections import Counter
import re

app = Flask(__name__)
CORS(app)

DATA_DIR = "data"


def load_cities_data():
    """ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ù† ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ù…"""
    with open(os.path.join(DATA_DIR, 'cities_landmarks.json'), 'r', encoding='utf-8') as f:
        return json.load(f)


class ProfessionalScriptGenerator:
    """Ù…ÙˆÙ„Ù‘Ø¯ Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© Ø§Ø­ØªØ±Ø§ÙÙŠØ©"""
    
    def __init__(self, city: str, landmark: str, duration_seconds: int):
        self.data = load_cities_data()
        self.city = city
        self.landmark = landmark
        self.duration_seconds = duration_seconds
        
        # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª (2.5 ÙƒÙ„Ù…Ø©/Ø«Ø§Ù†ÙŠØ© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©)
        self.target_words = int(duration_seconds * 2.5)
        
        # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù„Ù… ÙˆØ§Ù„Ù…Ø¯ÙŠÙ†Ø©
        self.city_data = self.data['cities'][city]
        self.landmark_data = self.city_data['landmarks'][landmark]
        
    def generate(self):
        """ØªÙˆÙ„ÙŠØ¯ Ø³ÙƒØ±ÙŠØ¨Øª Ø¥Ø¹Ù„Ø§Ù…ÙŠ Ù…Ø­ØªØ±Ù"""
        
        script_sections = []
        
        # 1. Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ù‚Ø¯Ù…Ø©
        title, intro = self._generate_intro()
        script_sections.append(("Ø§Ù„Ø¹Ù†ÙˆØ§Ù†", title))
        script_sections.append(("Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©", intro))
        
        # 2. Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
        if self.duration_seconds >= 90:
            historical = self._generate_historical_context()
            script_sections.append(("Ø§Ù„Ø®Ù„ÙÙŠØ© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©", historical))
        
        # 3. Ø§Ù„ÙˆØµÙ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ
        description = self._generate_detailed_description()
        script_sections.append(("Ø§Ù„ÙˆØµÙ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ", description))
        
        # 4. Ø§Ù„Ø£Ù‡Ù…ÙŠØ© ÙˆØ§Ù„ØªÙØ±Ø¯
        if self.duration_seconds >= 60:
            significance = self._generate_significance()
            script_sections.append(("Ø§Ù„Ø£Ù‡Ù…ÙŠØ© ÙˆØ§Ù„ØªÙØ±Ø¯", significance))
        
        # 5. Ù‚ØµØ© Ø±Ø¦ÙŠØ³ÙŠØ©
        if self.duration_seconds >= 120:
            stories = self.landmark_data.get('stories', [])
            if stories:
                story = random.choice(stories)
                story_section = self._format_story(story)
                script_sections.append(("Ù‚ØµØ©: " + story['title'], story_section))
        
        # 6. Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø¨Ø§Ø±Ø²Ø©
        if self.duration_seconds >= 90:
            key_landmarks = self._generate_key_landmarks()
            if key_landmarks:
                script_sections.append(("Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø¨Ø§Ø±Ø²Ø©", key_landmarks))
        
        # 7. Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        if self.duration_seconds >= 150:
            stats = self._generate_statistics()
            if stats:
                script_sections.append(("Ø£Ø±Ù‚Ø§Ù… ÙˆØ¥Ø­ØµØ§Ø¦ÙŠØ§Øª", stats))
        
        # 8. Ø²ÙˆØ§ÙŠØ§ ØµØ­ÙÙŠØ© Ù…Ù‚ØªØ±Ø­Ø©
        if self.duration_seconds >= 180:
            angles = self._suggest_journalist_angles()
            if angles:
                script_sections.append(("Ø²ÙˆØ§ÙŠØ§ ØµØ­ÙÙŠØ© Ù…Ù‚ØªØ±Ø­Ø©", angles))
        
        # 9. Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ø²ÙŠØ§Ø±Ø©
        if self.duration_seconds >= 60:
            visit_info = self._generate_visit_info()
            script_sections.append(("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø²ÙŠØ§Ø±Ø©", visit_info))
        
        # 10. Ø§Ù„Ø®Ø§ØªÙ…Ø©
        conclusion = self._generate_conclusion()
        script_sections.append(("Ø§Ù„Ø®Ø§ØªÙ…Ø©", conclusion))
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„
        full_script = self._build_full_script(script_sections)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª
        analysis = self._analyze_script(full_script)
        
        return {
            'success': True,
            'script': {
                'title': title,
                'full_text': full_script,
                'sections': [{"title": s[0], "content": s[1]} for s in script_sections],
                'city': self.city_data['name'],
                'city_region': self.city_data.get('region', ''),
                'landmark': self.landmark_data['name'],
                'landmark_type': self.landmark_data['type'],
                'landmark_category': self.landmark_data.get('category', ''),
                'duration_seconds': self.duration_seconds,
                'word_count': len(full_script.split()),
                'char_count': len(full_script),
                'estimated_reading_time': f"{int(len(full_script.split()) / 150)} Ø¯Ù‚ÙŠÙ‚Ø©",
                'generated_with': 'Professional Journalist Generator',
                'generated_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                
                # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠÙŠÙ†
                'field_questions': self.landmark_data.get('field_questions', []),
                'cultural_notes': self.landmark_data.get('cultural_notes', []),
                'stories': self.landmark_data.get('stories', []),
                'journalist_angles': self.landmark_data.get('journalist_angles', []),
                'statistics': self.landmark_data.get('statistics', {}),
                'references': self.landmark_data.get('references', []),
                'key_landmarks': self.landmark_data.get('key_landmarks', []),
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ
                'analysis': analysis
            }
        }
    
    def _generate_intro(self):
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ù‚Ø¯Ù…Ø©"""
        
        # Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        title_templates = [
            f"{self.landmark_data['name']}: {self.landmark_data['category']}",
            f"{self.landmark_data['name']} ÙÙŠ {self.city_data['name']} - {self.landmark_data['description']}",
            f"ØªÙ‚Ø±ÙŠØ±: {self.landmark_data['name']} - {self.landmark_data['type']}"
        ]
        title = random.choice(title_templates)
        
        # Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©
        intro_templates = [
            f"ÙÙŠ {self.city_data['name']}ØŒ {self.city_data['region']}ØŒ ÙŠÙ‚Ø¹ {self.landmark_data['name']}ØŒ {self.landmark_data['description']}. {self.landmark_data.get('detailed_info', '')}",
            
            f"ÙŠÙØ¹Ø¯ {self.landmark_data['name']} Ø£Ø­Ø¯ Ø£Ø¨Ø±Ø² Ø§Ù„Ù…Ø¹Ø§Ù„Ù… ÙÙŠ {self.city_data['name']}ØŒ Ø­ÙŠØ« {self.landmark_data['description']}. {self.landmark_data.get('detailed_info', '')}",
            
            f"{self.landmark_data['name']} - {self.landmark_data['category']} - Ù…Ø¹Ù„Ù… Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠ ÙÙŠ {self.city_data['name']} {self.landmark_data['description']}. {self.landmark_data.get('detailed_info', '')}"
        ]
        
        intro = random.choice(intro_templates)
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙŠÙˆÙ†Ø³ÙƒÙˆ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
        info = self.landmark_data.get('info', {})
        if info.get('unesco'):
            year = info.get('year_inscribed', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')
            unesco_criteria = info.get('unesco_criteria', '')
            unesco_name = info.get('inscription_name', self.landmark_data['name'])
            intro += f" Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…ÙØ¯Ø±Ø¬ ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ±Ø§Ø« Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ Ù„Ù„ÙŠÙˆÙ†Ø³ÙƒÙˆ Ù…Ù†Ø° Ø¹Ø§Ù… {year}Ù… ØªØ­Øª Ø§Ø³Ù… '{unesco_name}'"
            if unesco_criteria:
                intro += f" ÙˆÙÙ‚Ø§Ù‹ Ù„Ù€{unesco_criteria}"
            intro += "."
        
        return title, intro
    
    def _generate_historical_context(self):
        """Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ"""
        
        historical = self.landmark_data.get('historical_significance', '')
        
        if not historical:
            # ØªÙˆÙ„ÙŠØ¯ Ø³ÙŠØ§Ù‚ Ø¹Ø§Ù…
            age = self.landmark_data.get('info', {}).get('age', '')
            if age:
                historical = f"ÙŠØ¹ÙˆØ¯ ØªØ§Ø±ÙŠØ® {self.landmark_data['name']} Ø¥Ù„Ù‰ {age}ØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„Ù‡ Ø´Ø§Ù‡Ø¯Ø§Ù‹ Ø¹Ù„Ù‰ Ø­Ù‚Ø¨ ØªØ§Ø±ÙŠØ®ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø©."
        
        return historical
    
    def _generate_detailed_description(self):
        """Ø§Ù„ÙˆØµÙ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ"""
        
        parts = []
        
        # Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø£Ùˆ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
        arch_features = self.landmark_data.get('architectural_features', {})
        if arch_features:
            parts.append("Ù…Ù† Ø§Ù„Ù†Ø§Ø­ÙŠØ© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©:")
            for feature, desc in list(arch_features.items())[:3]:
                parts.append(f"- {feature.replace('_', ' ').title()}: {desc}")
        
        # Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø£Ø«Ø±ÙŠØ©
        arch_importance = self.landmark_data.get('archaeological_importance', '')
        if arch_importance:
            parts.append(arch_importance)
        
        # Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ
        development = self.landmark_data.get('historical_development', '')
        if development:
            parts.append(development)
        
        if not parts:
            parts.append(self.landmark_data['detailed_info'])
        
        return "\n\n".join(parts)
    
    def _generate_significance(self):
        """Ø§Ù„Ø£Ù‡Ù…ÙŠØ© ÙˆØ§Ù„ØªÙØ±Ø¯"""
        
        significance_parts = []
        
        # Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
        hist_sig = self.landmark_data.get('historical_significance', '')
        if hist_sig:
            significance_parts.append(f"ØªØ§Ø±ÙŠØ®ÙŠØ§Ù‹: {hist_sig}")
        
        # Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ©
        if self.landmark_data.get('info', {}).get('unesco'):
            significance_parts.append(f"Ø«Ù‚Ø§ÙÙŠØ§Ù‹: ÙŠÙ…Ø«Ù„ {self.landmark_data['name']} Ù‚ÙŠÙ…Ø© Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ø³ØªØ«Ù†Ø§Ø¦ÙŠØ©ØŒ ÙƒÙ…Ø§ Ø£Ù‚Ø±ØªÙ‡ Ù…Ù†Ø¸Ù…Ø© Ø§Ù„ÙŠÙˆÙ†Ø³ÙƒÙˆ.")
        
        # Ø§Ù„Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©
        stats = self.landmark_data.get('statistics', {})
        if stats.get('annual_visitors_2023'):
            significance_parts.append(f"Ø§Ù‚ØªØµØ§Ø¯ÙŠØ§Ù‹: Ø§Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ù…ÙˆÙ‚Ø¹ {stats['annual_visitors_2023']} Ø²Ø§Ø¦Ø± ÙÙŠ 2023ØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„Ù‡ Ù…Ø­Ø±ÙƒØ§Ù‹ Ø§Ù‚ØªØµØ§Ø¯ÙŠØ§Ù‹ Ù…Ù‡Ù…Ø§Ù‹.")
        
        return "\n\n".join(significance_parts) if significance_parts else ""
    
    def _format_story(self, story):
        """ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù‚ØµØ©"""
        
        formatted = story['content']
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
        if story.get('year'):
            formatted = f"[{story['year']}]\n\n{formatted}"
        
        if story.get('sources'):
            sources = ', '.join(story['sources'][:3])
            formatted += f"\n\nØ§Ù„Ù…ØµØ§Ø¯Ø±: {sources}"
        
        if story.get('type'):
            formatted += f"\n\nØ§Ù„Ù†ÙˆØ¹: {story['type']}"
        
        return formatted
    
    def _generate_key_landmarks(self):
        """Ø§Ù„Ù…Ø¹Ø§Ù„Ù… Ø§Ù„Ø¨Ø§Ø±Ø²Ø©"""
        
        key_landmarks = self.landmark_data.get('key_landmarks', [])
        
        if not key_landmarks:
            return ""
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø¹Ø¯Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø©
        num = min(len(key_landmarks), max(2, int(self.duration_seconds / 60)))
        selected = random.sample(key_landmarks, num)
        
        parts = []
        for lm in selected:
            desc = f"â€¢ {lm['name']}"
            if lm.get('year'):
                desc += f" ({lm['year']})"
            desc += f": {lm['description']}"
            
            if lm.get('significance'):
                desc += f" Ø§Ù„Ø£Ù‡Ù…ÙŠØ©: {lm['significance']}"
            
            parts.append(desc)
        
        return "\n\n".join(parts)
    
    def _generate_statistics(self):
        """Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        
        stats = self.landmark_data.get('statistics', {})
        
        if not stats:
            return ""
        
        parts = ["Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ØªØªØ­Ø¯Ø«:"]
        
        for key, value in stats.items():
            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…ÙØªØ§Ø­
            key_formatted = key.replace('_', ' ').title()
            parts.append(f"â€¢ {key_formatted}: {value}")
        
        return "\n".join(parts)
    
    def _suggest_journalist_angles(self):
        """Ø²ÙˆØ§ÙŠØ§ ØµØ­ÙÙŠØ© Ù…Ù‚ØªØ±Ø­Ø©"""
        
        angles = self.landmark_data.get('journalist_angles', [])
        
        if not angles:
            return ""
        
        # Ø§Ø®ØªÙŠØ§Ø± 3-5 Ø²ÙˆØ§ÙŠØ§ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹
        num = min(len(angles), 5)
        selected = random.sample(angles, num)
        
        parts = ["Ø²ÙˆØ§ÙŠØ§ ØµØ­ÙÙŠØ© Ù…Ù‚ØªØ±Ø­Ø© Ù„Ù„ØªØºØ·ÙŠØ©:"]
        for i, angle in enumerate(selected, 1):
            parts.append(f"{i}. {angle}")
        
        return "\n".join(parts)
    
    def _generate_visit_info(self):
        """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø²ÙŠØ§Ø±Ø©"""
        
        info = self.landmark_data.get('info', {})
        
        parts = ["Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ø²ÙŠØ§Ø±Ø©:"]
        
        if info.get('best_time'):
            parts.append(f"â€¢ Ø£ÙØ¶Ù„ ÙˆÙ‚Øª: {info['best_time']}")
        
        if info.get('duration'):
            parts.append(f"â€¢ Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©: {info['duration']}")
        
        if info.get('entry_fee'):
            parts.append(f"â€¢ Ø±Ø³ÙˆÙ… Ø§Ù„Ø¯Ø®ÙˆÙ„: {info['entry_fee']}")
        
        if info.get('guided_tours'):
            parts.append(f"â€¢ Ø§Ù„Ø¬ÙˆÙ„Ø§Øª Ø§Ù„Ù…ØµØ­ÙˆØ¨Ø©: {info['guided_tours']}")
        
        return "\n".join(parts)
    
    def _generate_conclusion(self):
        """Ø§Ù„Ø®Ø§ØªÙ…Ø©"""
        
        conclusions = [
            f"ÙŠØ¸Ù„ {self.landmark_data['name']} Ø´Ø§Ù‡Ø¯Ø§Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¸Ù…Ø© Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„Ø­Ø¶Ø§Ø±Ø©ØŒ ÙˆÙ…Ø¹Ù„Ù…Ø§Ù‹ ÙŠØ³ØªØ­Ù‚ Ø§Ù„Ø²ÙŠØ§Ø±Ø© ÙˆØ§Ù„Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ù…Ø¹Ù…Ù‚Ø©.",
            
            f"Ø¨ÙŠÙ† Ø§Ù„Ù…Ø§Ø¶ÙŠ ÙˆØ§Ù„Ø­Ø§Ø¶Ø±ØŒ ÙŠÙ‚Ù {self.landmark_data['name']} ÙƒØ¬Ø³Ø± ÙŠØ±Ø¨Ø· Ø§Ù„Ø£Ø¬ÙŠØ§Ù„ØŒ ÙˆÙŠØ±ÙˆÙŠ Ù‚ØµØ© {self.city_data['name']} Ø§Ù„Ø¹Ø±ÙŠÙ‚Ø©.",
            
            f"ÙŠÙˆØ§ØµÙ„ {self.landmark_data['name']} Ø¬Ø°Ø¨ Ø§Ù„Ø¨Ø§Ø­Ø«ÙŠÙ† ÙˆØ§Ù„Ø²ÙˆØ§Ø± Ù…Ù† Ù…Ø®ØªÙ„Ù Ø£Ù†Ø­Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ Ù…Ø¤ÙƒØ¯Ø§Ù‹ Ù…ÙƒØ§Ù†ØªÙ‡ ÙƒØ£Ø­Ø¯ Ø£Ù‡Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ù… ÙÙŠ Ø§Ù„Ù…Ù…Ù„ÙƒØ©.",
            
            f"ÙÙŠ Ø¹ØµØ± Ø§Ù„ØªØ­ÙˆÙ„ ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±ØŒ ÙŠØ­Ø§ÙØ¸ {self.landmark_data['name']} Ø¹Ù„Ù‰ Ù‡ÙˆÙŠØªÙ‡ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©ØŒ Ù…Ù‚Ø¯Ù…Ø§Ù‹ Ù„Ù„Ø¹Ø§Ù„Ù… Ù†Ù…ÙˆØ°Ø¬Ø§Ù‹ ÙØ±ÙŠØ¯Ø§Ù‹ Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±Ø§Ø«."
        ]
        
        return random.choice(conclusions)
    
    def _build_full_script(self, sections):
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„"""
        
        # Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¨Ø¯ÙˆÙ† Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø£Ù‚Ø³Ø§Ù…
        parts = []
        for title, content in sections:
            if title != "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†":  # ØªØ®Ø·ÙŠ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙÙŠ Ø§Ù„Ù†Øµ
                parts.append(content)
        
        return "\n\n".join(parts)
    
    def _analyze_script(self, script):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª"""
        
        words = script.split()
        sentences = script.split('.')
        
        # Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆØ§Ù„Ø¬Ù…Ù„
        word_count = len(words)
        sentence_count = len([s for s in sentences if s.strip()])
        
        # Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø©
        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0
        
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
        word_freq = Counter([w.strip('.,!?Ø›:') for w in words if len(w) > 3])
        most_common = word_freq.most_common(10)
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©
        readability = "Ø³Ù‡Ù„" if avg_sentence_length < 15 else "Ù…ØªÙˆØ³Ø·" if avg_sentence_length < 25 else "Ù…ØªÙ‚Ø¯Ù…"
        
        return {
            'word_count': word_count,
            'sentence_count': sentence_count,
            'avg_sentence_length': round(avg_sentence_length, 1),
            'readability': readability,
            'most_common_words': most_common[:5],
            'paragraph_count': script.count('\n\n') + 1
        }


class AdvancedScriptAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ ØªÙˆØµÙŠØ§Øª Ø§Ø­ØªØ±Ø§ÙÙŠØ©"""
    
    def __init__(self, script_text: str):
        self.script_text = script_text
        self.words = script_text.split()
        self.sentences = [s.strip() for s in script_text.split('.') if s.strip()]
        
    def analyze(self):
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù…Ø¹ ØªÙˆØµÙŠØ§Øª"""
        
        # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        basic_analysis = self._basic_analysis()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨
        style_analysis = self._style_analysis()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_analysis = self._content_analysis()
        
        # Ø§Ù„ØªÙˆØµÙŠØ§Øª
        recommendations = self._generate_recommendations(basic_analysis, style_analysis, content_analysis)
        
        # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„
        overall_score = self._calculate_overall_score(basic_analysis, style_analysis, content_analysis)
        
        return {
            'success': True,
            'analysis': {
                'basic': basic_analysis,
                'style': style_analysis,
                'content': content_analysis,
                'recommendations': recommendations,
                'overall_score': overall_score,
                'analyzed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        }
    
    def _basic_analysis(self):
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"""
        
        word_count = len(self.words)
        sentence_count = len(self.sentences)
        char_count = len(self.script_text)
        paragraph_count = self.script_text.count('\n\n') + 1
        
        avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0
        avg_word_length = sum(len(w) for w in self.words) / word_count if word_count > 0 else 0
        
        # ÙˆÙ‚Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù‚Ø¯Ø± (150 ÙƒÙ„Ù…Ø©/Ø¯Ù‚ÙŠÙ‚Ø©)
        reading_time = word_count / 150
        
        # ÙˆÙ‚Øª Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ù‚Ø¯Ø± (2.5 ÙƒÙ„Ù…Ø©/Ø«Ø§Ù†ÙŠØ©)
        broadcast_time = word_count / 2.5
        
        return {
            'word_count': word_count,
            'sentence_count': sentence_count,
            'char_count': char_count,
            'paragraph_count': paragraph_count,
            'avg_sentence_length': round(avg_sentence_length, 1),
            'avg_word_length': round(avg_word_length, 1),
            'reading_time_minutes': round(reading_time, 1),
            'broadcast_time_seconds': int(broadcast_time)
        }
    
    def _style_analysis(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨"""
        
        # Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠ
        journalist_indicators = [
            'ÙŠÙ‚Ø¹', 'ØªÙ‚Ø¹', 'ÙŠÙØ¹Ø¯', 'ØªÙØ¹Ø¯', 'ÙŠØ´ÙŠØ±', 'ØªØ´ÙŠØ±', 'ÙˆÙÙ‚Ø§Ù‹', 
            'Ø­ÙŠØ«', 'Ø¥Ø°', 'Ø¨ÙŠÙ†Ù…Ø§', 'Ù…Ù† Ø¬Ù‡Ø©', 'Ù…Ù† Ù†Ø§Ø­ÙŠØ©', 'ØªØ§Ø±ÙŠØ®ÙŠØ§Ù‹', 
            'Ø­Ø§Ù„ÙŠØ§Ù‹', 'Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹', 'Ø§Ù„Ù…ØµØ§Ø¯Ø±', 'Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±', 'Ø§Ù„Ø¯Ø±Ø§Ø³Ø§Øª'
        ]
        
        journalist_score = sum(1 for word in journalist_indicators if word in self.script_text)
        
        # Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ© (ØºÙŠØ§Ø¨ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø´Ø®ØµÙŠØ©)
        personal_pronouns = ['Ø£Ù†Ø§', 'Ù†Ø­Ù†', 'Ø£Ø´Ø¹Ø±', 'Ø£Ø±Ù‰', 'Ø£Ø¹ØªÙ‚Ø¯']
        objectivity_score = 100 - (sum(1 for word in personal_pronouns if word in self.script_text) * 10)
        objectivity_score = max(0, min(100, objectivity_score))
        
        # Ø§Ù„Ø¯Ù‚Ø© (ÙˆØ¬ÙˆØ¯ Ø£Ø±Ù‚Ø§Ù… ÙˆØªÙˆØ§Ø±ÙŠØ®)
        numbers = len(re.findall(r'\d+', self.script_text))
        has_dates = len(re.findall(r'\d{4}', self.script_text)) > 0
        
        # Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ© (Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ù…Ù†Ø¸Ù…Ø©)
        has_structure = self.script_text.count('\n\n') >= 2
        
        return {
            'style_type': 'Ø¥Ø¹Ù„Ø§Ù…ÙŠ Ù…Ø­ØªØ±Ù' if journalist_score >= 5 else 'Ø¹Ø§Ù…',
            'journalist_score': journalist_score,
            'objectivity': f"{objectivity_score}%",
            'uses_data': numbers > 0,
            'numbers_count': numbers,
            'has_dates': has_dates,
            'well_structured': has_structure,
            'tone': self._detect_tone()
        }
    
    def _detect_tone(self):
        """Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù†Ø¨Ø±Ø©"""
        
        formal_words = ['ÙŠÙØ¹Ø¯', 'ØªÙØ¹ØªØ¨Ø±', 'ÙŠØ´ÙŠØ±', 'ÙˆÙÙ‚Ø§Ù‹', 'Ø­ÙŠØ«', 'Ø¥Ø°']
        informal_words = ['Ø±Ø§Ø¦Ø¹', 'Ø¬Ù…ÙŠÙ„', 'Ù…Ø°Ù‡Ù„', 'Ø®ÙŠØ§Ù„ÙŠ']
        
        formal_count = sum(1 for word in formal_words if word in self.script_text)
        informal_count = sum(1 for word in informal_words if word in self.script_text)
        
        if formal_count > informal_count * 2:
            return "Ø±Ø³Ù…ÙŠ"
        elif informal_count > formal_count:
            return "ØºÙŠØ± Ø±Ø³Ù…ÙŠ"
        else:
            return "Ù…ØªÙˆØ§Ø²Ù†"
    
    def _content_analysis(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        
        # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
        word_freq = Counter([w.strip('.,!?Ø›:') for w in self.words if len(w) > 3])
        most_common = word_freq.most_common(10)
        
        # Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        topics = []
        topic_keywords = {
            'ØªØ§Ø±ÙŠØ®': ['ØªØ§Ø±ÙŠØ®', 'Ù‚Ø¯ÙŠÙ…', 'Ø¹ØµØ±', 'Ø­Ù‚Ø¨Ø©', 'Ù‚Ø±Ù†'],
            'Ø«Ù‚Ø§ÙØ©': ['Ø«Ù‚Ø§ÙØ©', 'ØªØ±Ø§Ø«', 'Ø¹Ù…Ø§Ø±Ø©', 'ÙÙ†'],
            'Ø³ÙŠØ§Ø­Ø©': ['Ø³ÙŠØ§Ø­Ø©', 'Ø²ÙˆØ§Ø±', 'Ø²ÙŠØ§Ø±Ø©', 'Ø¬ÙˆÙ„Ø©'],
            'Ø§Ù‚ØªØµØ§Ø¯': ['Ø§Ù‚ØªØµØ§Ø¯', 'Ø§Ø³ØªØ«Ù…Ø§Ø±', 'ØªÙ†Ù…ÙŠØ©', 'ÙˆØ¸Ø§Ø¦Ù']
        }
        
        for topic, keywords in topic_keywords.items():
            if any(kw in self.script_text for kw in keywords):
                topics.append(topic)
        
        # ÙˆØ¬ÙˆØ¯ Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ø£Ùˆ Ù…ØµØ§Ø¯Ø±
        has_quotes = '"' in self.script_text or "'" in self.script_text
        mentions_sources = any(word in self.script_text for word in ['Ø§Ù„Ù…ØµØ§Ø¯Ø±', 'Ø§Ù„Ù…Ø±Ø¬Ø¹', 'Ø§Ù„Ø¯Ø±Ø§Ø³Ø©', 'Ø§Ù„ØªÙ‚Ø±ÙŠØ±'])
        
        return {
            'most_common_words': [{'word': word, 'count': count} for word, count in most_common[:5]],
            'topics_covered': topics,
            'has_quotes': has_quotes,
            'mentions_sources': mentions_sources,
            'diversity_score': len(set(self.words)) / len(self.words) if self.words else 0
        }
    
    def _generate_recommendations(self, basic, style, content):
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
        
        recommendations = []
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„
        if basic['word_count'] < 100:
            recommendations.append({
                'type': 'Ø§Ù„Ø·ÙˆÙ„',
                'priority': 'Ø¹Ø§Ù„ÙŠØ©',
                'issue': 'Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹',
                'suggestion': 'Ø£Ø¶Ù Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ù„ÙÙŠØ©. Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠ Ø§Ù„Ø¬ÙŠØ¯ ÙŠØªØ±Ø§ÙˆØ­ Ø¨ÙŠÙ† 200-500 ÙƒÙ„Ù…Ø©.'
            })
        elif basic['word_count'] > 600:
            recommendations.append({
                'type': 'Ø§Ù„Ø·ÙˆÙ„',
                'priority': 'Ù…ØªÙˆØ³Ø·Ø©',
                'issue': 'Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ø·ÙˆÙŠÙ„ Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù…Ù…Ù„Ø§Ù‹',
                'suggestion': 'Ø­Ø§ÙˆÙ„ Ø§Ù„Ø§Ø®ØªØµØ§Ø± ÙˆØ§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©. Ù‚Ø³Ù‘Ù… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ù„Ù‰ ØªÙ‚Ø±ÙŠØ±ÙŠÙ† Ù…Ù†ÙØµÙ„ÙŠÙ† Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±.'
            })
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨
        if style['journalist_score'] < 3:
            recommendations.append({
                'type': 'Ø§Ù„Ø£Ø³Ù„ÙˆØ¨',
                'priority': 'Ø¹Ø§Ù„ÙŠØ©',
                'issue': 'Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ØºÙŠØ± Ø¥Ø¹Ù„Ø§Ù…ÙŠ Ø¨Ù…Ø§ ÙŠÙƒÙÙŠ',
                'suggestion': 'Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ø¨Ø§Ø±Ø§Øª Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ù…Ø«Ù„: "ÙˆÙÙ‚Ø§Ù‹ Ù„Ù„Ù…ØµØ§Ø¯Ø±"ØŒ "ØªØ´ÙŠØ± Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±"ØŒ "Ù…Ù† Ø§Ù„Ø¬Ø¯ÙŠØ± Ø¨Ø§Ù„Ø°ÙƒØ±". ØªØ¬Ù†Ø¨ Ø§Ù„Ø¶Ù…Ø§Ø¦Ø± Ø§Ù„Ø´Ø®ØµÙŠØ©.'
            })
        
        if int(style['objectivity'].rstrip('%')) < 80:
            recommendations.append({
                'type': 'Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©',
                'priority': 'Ø¹Ø§Ù„ÙŠØ©',
                'issue': 'Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¢Ø±Ø§Ø¡ Ø´Ø®ØµÙŠØ©',
                'suggestion': 'Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙŠØ§Ø¯ Ø§Ù„ØµØ­ÙÙŠ. Ø§Ø³ØªØ¨Ø¯Ù„ Ø§Ù„Ø¢Ø±Ø§Ø¡ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø¨Ø­Ù‚Ø§Ø¦Ù‚ ÙˆØ¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ«Ù‚Ø©.'
            })
        
        # ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        if not style['uses_data']:
            recommendations.append({
                'type': 'Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª',
                'priority': 'Ù…ØªÙˆØ³Ø·Ø©',
                'issue': 'Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø£Ø±Ù‚Ø§Ù… Ø£Ùˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª',
                'suggestion': 'Ø£Ø¶Ù Ø£Ø±Ù‚Ø§Ù…Ø§Ù‹ ÙˆØ¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù„Ø¯Ø¹Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª. Ù…Ø«Ù„: Ø¹Ø¯Ø¯ Ø§Ù„Ø²ÙˆØ§Ø±ØŒ Ø§Ù„ØªÙƒÙ„ÙØ©ØŒ Ø§Ù„Ù…Ø³Ø§Ø­Ø©ØŒ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.'
            })
        
        if not content['mentions_sources']:
            recommendations.append({
                'type': 'Ø§Ù„Ù…ØµØ§Ø¯Ø±',
                'priority': 'Ø¹Ø§Ù„ÙŠØ©',
                'issue': 'Ø¹Ø¯Ù… Ø°ÙƒØ± Ø§Ù„Ù…ØµØ§Ø¯Ø±',
                'suggestion': 'Ø§Ø°ÙƒØ± Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…ØµØ¯Ø§Ù‚ÙŠØ©. Ù…Ø«Ù„: "ÙˆÙÙ‚Ø§Ù‹ Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙŠÙˆÙ†Ø³ÙƒÙˆ"ØŒ "Ø­Ø³Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡ÙŠØ¦Ø©".'
            })
        
        if basic['paragraph_count'] < 3:
            recommendations.append({
                'type': 'Ø§Ù„Ù‡ÙŠÙƒÙ„Ø©',
                'priority': 'Ù…ØªÙˆØ³Ø·Ø©',
                'issue': 'Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª ØºÙŠØ± Ù…Ù†Ø¸Ù… ÙÙŠ ÙÙ‚Ø±Ø§Øª',
                'suggestion': 'Ù‚Ø³Ù‘Ù… Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª ÙˆØ§Ø¶Ø­Ø©: Ù…Ù‚Ø¯Ù…Ø©ØŒ Ø¬Ø³Ù… (2-3 ÙÙ‚Ø±Ø§Øª)ØŒ Ø®Ø§ØªÙ…Ø©.'
            })
        
        if basic['avg_sentence_length'] > 25:
            recommendations.append({
                'type': 'Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©',
                'priority': 'Ù…ØªÙˆØ³Ø·Ø©',
                'issue': 'Ø§Ù„Ø¬Ù…Ù„ Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹',
                'suggestion': 'Ø§Ø®ØªØµØ± Ø§Ù„Ø¬Ù…Ù„ Ù„ØªÙƒÙˆÙ† Ø£Ø³Ù‡Ù„ ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„ÙÙ‡Ù…. Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ©: 15-20 ÙƒÙ„Ù…Ø©.'
            })
        
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…Ø´Ø§ÙƒÙ„
        if not recommendations:
            recommendations.append({
                'type': 'Ø¹Ø§Ù…',
                'priority': 'Ù…Ø¹Ù„ÙˆÙ…Ø©',
                'issue': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø´Ø§ÙƒÙ„ ÙƒØ¨ÙŠØ±Ø©',
                'suggestion': 'Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ø­ØªØ±Ø§ÙÙŠ ÙˆØ¬ÙŠØ¯ Ø§Ù„Ø¨Ù†Ø§Ø¡. Ø§Ø³ØªÙ…Ø± Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆÙ‰!'
            })
        
        return recommendations
    
    def _calculate_overall_score(self, basic, style, content):
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„"""
        
        score = 0
        max_score = 100
        
        # Ø§Ù„Ø·ÙˆÙ„ (20 Ù†Ù‚Ø·Ø©)
        if 200 <= basic['word_count'] <= 500:
            score += 20
        elif 150 <= basic['word_count'] < 200 or 500 < basic['word_count'] <= 600:
            score += 15
        else:
            score += 10
        
        # Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù…ÙŠ (25 Ù†Ù‚Ø·Ø©)
        score += min(25, style['journalist_score'] * 3)
        
        # Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ÙŠØ© (15 Ù†Ù‚Ø·Ø©)
        objectivity = int(style['objectivity'].rstrip('%'))
        score += objectivity * 0.15
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (15 Ù†Ù‚Ø·Ø©)
        if style['uses_data']:
            score += 10
        if style['has_dates']:
            score += 5
        
        # Ø°ÙƒØ± Ø§Ù„Ù…ØµØ§Ø¯Ø± (10 Ù†Ù‚Ø·Ø©)
        if content['mentions_sources']:
            score += 10
        
        # Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ© (10 Ù†Ù‚Ø·Ø©)
        if basic['paragraph_count'] >= 3:
            score += 5
        if style['well_structured']:
            score += 5
        
        # Ø§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ù„ØºÙˆÙŠ (5 Ù†Ù‚Ø§Ø·)
        diversity = content['diversity_score']
        score += diversity * 5
        
        score = min(max_score, round(score, 1))
        
        # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØµÙŠ
        if score >= 90:
            rating = "Ù…Ù…ØªØ§Ø²"
        elif score >= 75:
            rating = "Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹"
        elif score >= 60:
            rating = "Ø¬ÙŠØ¯"
        elif score >= 50:
            rating = "Ù…Ù‚Ø¨ÙˆÙ„"
        else:
            rating = "ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†"
        
        return {
            'score': score,
            'rating': rating,
            'max_score': max_score
        }


# ============= API Endpoints =============

@app.route('/')
def home():
    return jsonify({
        'service': 'Professional Journalist Script Generator',
        'version': '4.0',
        'mode': 'Journalist Only',
        'features': [
            'Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª Ø¥Ø¹Ù„Ø§Ù…ÙŠØ© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø·ÙˆÙŠÙ„Ø©',
            'Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ³Ù‘Ø¹Ø© ÙˆØ­Ù‚ÙŠÙ‚ÙŠØ©',
            'ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ ØªÙˆØµÙŠØ§Øª',
            'Ù‚ØµØµ ØªØ§Ø±ÙŠØ®ÙŠØ© Ù…ÙØµÙ„Ø©',
            'Ø²ÙˆØ§ÙŠØ§ ØµØ­ÙÙŠØ© Ù…Ù‚ØªØ±Ø­Ø©'
        ]
    })


@app.route('/api/cities', methods=['GET'])
def get_cities():
    data = load_cities_data()
    
    cities_list = []
    for city_id, city_info in data['cities'].items():
        cities_list.append({
            'id': city_id,
            'name': city_info['name'],
            'name_en': city_info['name_en'],
            'icon': city_info['icon'],
            'description': city_info['description'],
            'region': city_info.get('region', ''),
            'population': city_info.get('population', ''),
            'landmarks_count': len(city_info['landmarks'])
        })
    
    return jsonify({'success': True, 'cities': cities_list})


@app.route('/api/cities/<city_id>/landmarks', methods=['GET'])
def get_landmarks(city_id):
    data = load_cities_data()
    
    if city_id not in data['cities']:
        return jsonify({'success': False, 'error': 'City not found'}), 404
    
    city = data['cities'][city_id]
    landmarks_list = []
    
    for landmark_id, landmark_info in city['landmarks'].items():
        landmarks_list.append({
            'id': landmark_id,
            'name': landmark_info['name'],
            'icon': landmark_info['icon'],
            'type': landmark_info['type'],
            'category': landmark_info.get('category', ''),
            'description': landmark_info['description'],
            'has_stories': len(landmark_info.get('stories', [])) > 0,
            'stories_count': len(landmark_info.get('stories', [])),
            'unesco': landmark_info.get('info', {}).get('unesco', False)
        })
    
    return jsonify({
        'success': True,
        'city': {
            'id': city_id,
            'name': city['name'],
            'icon': city['icon'],
            'region': city.get('region', '')
        },
        'landmarks': landmarks_list
    })


@app.route('/api/generate', methods=['POST'])
def generate_script():
    data = request.get_json()
    
    required_fields = ['city', 'landmark', 'duration']
    for field in required_fields:
        if field not in data:
            return jsonify({
                'success': False,
                'error': f'Missing required field: {field}'
            }), 400
    
    try:
        generator = ProfessionalScriptGenerator(
            city=data['city'],
            landmark=data['landmark'],
            duration_seconds=data['duration']
        )
        
        result = generator.generate()
        return jsonify(result)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/analyze', methods=['POST'])
def analyze_script():
    data = request.get_json()
    
    if 'script' not in data:
        return jsonify({
            'success': False,
            'error': 'Missing script text'
        }), 400
    
    try:
        analyzer = AdvancedScriptAnalyzer(data['script'])
        result = analyzer.analyze()
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


if __name__ == '__main__':
    print("=" * 80)
    print("ğŸ™ï¸  Professional Journalist Script Generator")
    print("=" * 80)
    print("ğŸ“° Mode: Journalist Only (Professional)")
    print("ğŸ“š Features: Long scripts + Advanced analysis + Recommendations")
    print("")
    
    data = load_cities_data()
    total_landmarks = sum(len(city['landmarks']) for city in data['cities'].values())
    total_stories = sum(
        len(landmark.get('stories', []))
        for city in data['cities'].values()
        for landmark in city['landmarks'].values()
    )
    
    print(f"ğŸ“Š Cities: {len(data['cities'])}")
    print(f"ğŸ“Š Landmarks: {total_landmarks}")
    print(f"ğŸ“– Stories: {total_stories}")
    print("\n" + "=" * 80)
    print("ğŸŒ Server: http://localhost:5000/")
    print("=" * 80 + "\n")
    
    app.run(host='0.0.0.0', port=5000, debug=True)
